"""
ContentRSS åç«¯ - AI æƒ…æŠ¥åˆ†ææœåŠ¡
åŸºäº Flask + OpenAI å…¼å®¹ API
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
from openai import OpenAI
import os
import json
import requests
from dotenv import load_dotenv
from datetime import datetime
from database import init_db, get_db_connection, get_placeholder, is_postgres
import hashlib
from typing import Any, Dict, List, Optional

# æ¡ä»¶å¯¼å…¥ PostgreSQL ä¸“ç”¨æ¨¡å—
if is_postgres():
    from psycopg2.extras import execute_values, RealDictCursor
    from psycopg2.pool import ThreadedConnectionPool

from services.entities import EntityService
from services.tag_service import tag_service
from topics import topic_service

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
OPENAI_BASE_URL = os.getenv('OPENAI_BASE_URL')
DEFAULT_MODEL = os.getenv('DEFAULT_MODEL', 'qwen-max')  # å‡çº§åˆ°æ›´å¼ºæ¨¡å‹
SPECIAL_API_URL = os.getenv('SPECIAL_API_URL')
SPECIAL_CHAIN_ID = int(os.getenv('SPECIAL_CHAIN_ID', '1036'))
ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')

# éªŒè¯é…ç½®
if not OPENAI_API_KEY:
    raise ValueError("ç¼ºå°‘ OPENAI_API_KEY é…ç½®")
if not OPENAI_BASE_URL:
    raise ValueError("ç¼ºå°‘ OPENAI_BASE_URL é…ç½®")

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
ai_client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL
)

# åˆ›å»º Flask åº”ç”¨
app = Flask(__name__)

# ç”Ÿäº§ç¯å¢ƒ CORS é…ç½®
# æ³¨æ„ï¼šstrip() å¤„ç†ç¯å¢ƒå˜é‡ä¸­å¯èƒ½çš„ç©ºæ ¼/æ¢è¡Œ
PROD_ORIGINS = [origin.strip() for origin in os.getenv('ALLOWED_ORIGINS', 'http://localhost:16889,http://localhost:5173').split(',')]
CORS(app, origins=PROD_ORIGINS, supports_credentials=True)

# ç»Ÿä¸€å“åº”å·¥å…·
from utils.response import success, error, not_found, bad_request, internal_error, register_error_handlers, validate_json
register_error_handlers(app)


def get_category_mapping() -> Dict[str, str]:
    """è·å–åˆ†ç±»æ˜ å°„ï¼ˆä»æ•°æ®åº“ï¼Œæ›¿ä»£ç¡¬ç¼–ç ï¼‰"""
    return tag_service.get_category_mapping()


def get_category_label(category_key: str) -> str:
    """è·å–åˆ†ç±»æ˜¾ç¤ºåç§°"""
    mapping = get_category_mapping()
    return mapping.get(category_key, category_key)


# ========== æ•°æ®åº“è¿æ¥ç®¡ç† (SQLite / PostgreSQL åŒæ¨¡å¼) ==========
from contextlib import contextmanager

DB_URL = os.getenv('DATABASE_URL')
_db_pool = None

def get_pool():
    global _db_pool
    if is_postgres():
        if _db_pool is None:
            print("ğŸ’¡ åˆå§‹åŒ– PostgreSQL è¿æ¥æ± ...")
            _db_pool = ThreadedConnectionPool(1, 10, DB_URL, sslmode='require')
        return _db_pool
    return None  # SQLite ä¸ä½¿ç”¨è¿æ¥æ± 

@contextmanager
def db_conn():
    if is_postgres():
        pool = get_pool()
        conn = pool.getconn()
        conn.cursor_factory = RealDictCursor
        try:
            yield conn
        finally:
            pool.putconn(conn)
    else:
        # SQLite æ¨¡å¼ï¼šæ¯æ¬¡è¯·æ±‚æ–°å»ºè¿æ¥
        conn = get_db_connection()
        try:
            yield conn
        finally:
            conn.close()

# ================================


entity_service = EntityService()


def _coerce_special_payload(data: Any) -> Dict[str, List[Dict[str, Any]]]:
    if isinstance(data, list):
        return {"insight": data}
    if isinstance(data, dict):
        return {k: v for k, v in data.items() if isinstance(v, list)}
    return {}


def parse_datetime(value: Any) -> Optional[datetime]:
    """è§£ææ—¥æœŸæ—¶é—´ï¼Œå…¼å®¹ datetime å¯¹è±¡å’Œå„ç§å­—ç¬¦ä¸²æ ¼å¼"""
    if value is None:
        return None
    if isinstance(value, datetime):
        return value
    if isinstance(value, str):
        # ç§»é™¤å¯èƒ½çš„å¾®ç§’æˆ–æ—¶åŒºåç¼€ä»¥ä¾¿ç»Ÿä¸€è§£æ
        clean_val = value.split('.')[0].replace('Z', '').replace('T', ' ')
        try:
            return datetime.strptime(clean_val, '%Y-%m-%d %H:%M:%S')
        except ValueError:
            try:
                return datetime.fromisoformat(value)
            except:
                return None
    return None


def get_raw_articles_by_category() -> Dict[str, List[Dict[str, Any]]]:
    """ç»Ÿä¸€è·å–æ‰€æœ‰åˆ†ç±»çš„åŸå§‹æ–‡ç« æ•°æ®ï¼ˆæè‡´ä¼˜åŒ–ï¼šç¼“å­˜å‘½ä¸­ä»… 1 æ¬¡ RTTï¼‰"""
    category_keys = list(get_category_mapping().keys())
    
    # 1. å•æ¬¡æŸ¥è¯¢è·å–æ•°æ® + çŠ¶æ€
    db_data, last_sync_map = fetch_all_raw_articles_with_metadata(category_keys)
    
    # 2. æ£€æŸ¥åŒæ­¥éœ€æ±‚ (æ¯å¤©ä»…éœ€åŒæ­¥ä¸€æ¬¡)
    now = datetime.now()
    needs_sync_keys = []
    
    for key in category_keys:
        last_sync = last_sync_map.get(key)
        # å¦‚æœæ²¡æ•°æ®ï¼Œæˆ–è€…ä¸Šæ¬¡åŒæ­¥ä¸æ˜¯ä»Šå¤©
        if not last_sync or last_sync.date() < now.date():
            needs_sync_keys.append(key)
    
    if needs_sync_keys:
        print(f"ğŸ”„ éœ€è¦åŒæ­¥ ({len(needs_sync_keys)}ä¸ªåˆ†ç±»): {needs_sync_keys}")
        try:
            response = fetch_special_data()
            sync_data = _coerce_special_payload(parse_special_response(response))
            for key in needs_sync_keys:
                items = sync_data.get(key) or []
                if items:
                    persist_raw_items(key, items)
            # åªæœ‰åŒæ­¥å‘ç”Ÿæ—¶æ‰è¿›è¡Œç¬¬äºŒæ¬¡æŸ¥è¯¢
            db_data, _ = fetch_all_raw_articles_with_metadata(category_keys)
        except Exception as e:
            print(f"âš ï¸ åŒæ­¥å¤±è´¥: {e}")

    return db_data

def get_articles_for_category(category_key: str) -> List[Dict[str, Any]]:
    """è·å–å•åˆ†ç±»æ•°æ®ï¼ˆå¤ç”¨æ‰¹é‡é€»è¾‘ä»¥èŠ‚çœè¿æ¥ï¼‰"""
    data = get_raw_articles_by_category()
    return data.get(category_key, [])


def fetch_special_category_items(category_key: str) -> List[Dict[str, Any]]:
    response = fetch_special_data()
    data = _coerce_special_payload(parse_special_response(response))
    return data.get(category_key, []) if data else []


def _row_value(row: Any, key: str) -> Any:
    if isinstance(row, dict):
        return row.get(key)
    try:
        return row[key]
    except Exception:
        return None


def fetch_all_raw_articles_with_metadata(category_keys: List[str], limit_per_cat: int = 40) -> tuple[Dict[str, List[Dict[str, Any]]], Dict[str, datetime]]:
    """æ‰¹é‡ä»æ•°æ®åº“è¯»å–æ–‡ç« å’Œæœ€æ–°çš„åŒæ­¥æ—¶é—´"""
    try:
        if not category_keys:
            return {}, {}
        
        ph = get_placeholder()  # SQLite: ? / PostgreSQL: %s
        placeholders = ", ".join([ph] * len(category_keys))
        
        # SQLite å’Œ PostgreSQL éƒ½æ”¯æŒ WITH å’Œ ROW_NUMBER() çª—å£å‡½æ•°
        query = f"""
            WITH ranked_articles AS (
                SELECT 
                    category_key, 
                    raw_payload,
                    ingested_at,
                    ROW_NUMBER() OVER(PARTITION BY category_key ORDER BY ingested_at DESC) as rank
                FROM raw_articles
                WHERE category_key IN ({placeholders})
            )
            SELECT category_key, raw_payload, ingested_at, rank
            FROM ranked_articles
            WHERE rank <= {ph}
        """
        
        db_data: Dict[str, List[Dict[str, Any]]] = {key: [] for key in category_keys}
        last_sync_map: Dict[str, datetime] = {}
        
        with db_conn() as conn:
            cur = conn.cursor()
            cur.execute(query, (*category_keys, limit_per_cat))
            rows = cur.fetchall()
            
        for row in rows:
            cat = row["category_key"]
            payload = row["raw_payload"]
            ingested_at = row["ingested_at"]
            rank = row["rank"]
            
            # è®°å½•è¯¥åˆ†ç±»æœ€æ–°çš„åŒæ­¥æ—¶é—´ï¼ˆrank=1 çš„å³ä¸ºæœ€æ–°ï¼‰
            if rank == 1:
                last_sync_map[cat] = parse_datetime(ingested_at)
            
            if isinstance(payload, str):
                try:
                    payload = json.loads(payload)
                except:
                    payload = {"raw_payload": payload}
            
            if isinstance(payload, dict):
                # å°† ingested_at æ·»åŠ åˆ°è¿”å›æ•°æ®ä¸­
                payload["ingested_at"] = str(ingested_at) if ingested_at else None
                db_data[cat].append(payload)
                
        return db_data, last_sync_map
    except Exception as e:
        print(f"âš ï¸ æ‰¹é‡è¯»å– raw_articles å¤±è´¥: {e}")
        return {}, {}


def get_all_synced_recently(category_keys: List[str]) -> List[str]:
    """æ‰¹é‡æ£€æŸ¥å“ªäº›åˆ†ç±»ä»Šå¤©å·²ç»åŒæ­¥è¿‡"""
    _, last_sync_map = fetch_all_raw_articles_with_metadata(category_keys, limit_per_cat=1)
    today = datetime.now().date()
    return [k for k, v in last_sync_map.items() if v.date() >= today]


def is_synced_recently(category_key: str) -> bool:
    """å•ä¸ªæ£€æŸ¥"""
    return category_key in get_all_synced_recently([category_key])



def persist_raw_items(category_key: str, items: List[Dict[str, Any]]) -> None:
    """æ‰¹é‡ä¿å­˜æ–‡ç« ï¼Œæ”¯æŒ SQLite å’Œ PostgreSQL"""
    if not items:
        return
    try:
        data_to_insert = []
        for item in items:
            normalized = normalize_article(item, category_key)
            if not normalized:
                continue
            payload = json.dumps(item, ensure_ascii=False)
            data_to_insert.append((
                normalized["source_name"],
                normalized["source_url"],
                normalized["title"],
                normalized["summary"],
                normalized["content"],
                category_key,
                payload,
                None # published_at
            ))

        if not data_to_insert:
            return

        with db_conn() as conn:
            cur = conn.cursor()
            if is_postgres():
                # PostgreSQL: ä½¿ç”¨ execute_values æ‰¹é‡æ’å…¥
                query = """
                    INSERT INTO raw_articles 
                    (source_name, source_url, title, summary, content, category_key, raw_payload, published_at) 
                    VALUES %s
                    ON CONFLICT (source_url) DO UPDATE SET 
                    source_name = EXCLUDED.source_name, 
                    title = EXCLUDED.title, 
                    summary = EXCLUDED.summary, 
                    content = EXCLUDED.content, 
                    category_key = EXCLUDED.category_key, 
                    raw_payload = EXCLUDED.raw_payload, 
                    published_at = EXCLUDED.published_at, 
                    ingested_at = CURRENT_TIMESTAMP
                """
                execute_values(cur, query, data_to_insert)
            else:
                # SQLite: é€æ¡æ’å…¥ (INSERT OR REPLACE)
                query = """
                    INSERT OR REPLACE INTO raw_articles 
                    (source_name, source_url, title, summary, content, category_key, raw_payload, published_at) 
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """
                cur.executemany(query, data_to_insert)
            conn.commit()
            print(f"âœ… å·²å­˜å…¥ {len(data_to_insert)} æ¡æ•°æ®åˆ°åˆ†ç±» {category_key}")
    except Exception as e:
        print(f"âš ï¸ æ‰¹é‡ä¿å­˜ raw_articles å¤±è´¥: {e}")


def safe_json(value: Any) -> Dict[str, Any]:
    if isinstance(value, dict):
        return value
    if isinstance(value, str):
        try:
            return json.loads(value)
        except Exception:
            return {}
    return {}


def normalize_article(article: Dict[str, Any], category_key: str) -> Optional[Dict[str, Any]]:
    if not isinstance(article, dict):
        return None
        
    fields = article.get("fields", article)
    raw_info = fields.get("æ–‡ç« ä¿¡æ¯") or fields.get("article_info") or fields.get("info")
    info = safe_json(raw_info) if raw_info else {}

    title = info.get("æ–‡ç« æ ‡é¢˜") or fields.get("æ–‡ç« æ ‡é¢˜-mossç”¨") or fields.get("title") or ""
    summary = info.get("æ‘˜è¦") or fields.get("æ‘˜è¦") or fields.get("summary") or ""
    content = fields.get("æ–‡ç« å†…å®¹") or fields.get("content") or summary or ""
    source_name = info.get("ä½œè€…åç§°") or fields.get("source_name") or fields.get("source") or ""
    source_url = info.get("æ–‡ç« URL") or fields.get("source_url") or fields.get("url")
    raw_id = fields.get("è‡ªå¢ID") or fields.get("id") or fields.get("article_id")

    if not title:
        return None

    try:
        article_id = int(raw_id) if raw_id is not None else None
    except Exception:
        article_id = None

    if article_id is None:
        id_source = f"{title}|{source_url or ''}"
        article_id = int(hashlib.md5(id_source.encode("utf-8")).hexdigest()[:8], 16)

    return {
        "id": article_id,
        "title": title,
        "summary": summary,
        "content": content,
        "source_name": source_name,
        "source_url": source_url,
        "category_key": category_key,
    }


def build_summary_payload(analysis: Dict[str, Any], raw_summary: str) -> str:
    polarity = analysis.get("polarity", "neutral")
    sentiment = "neutral"
    if polarity == "positive":
        sentiment = "bullish"
    elif polarity == "negative":
        sentiment = "bearish"

    thesis = analysis.get("opinion") or analysis.get("fact") or raw_summary or "æš‚æ— æ‘˜è¦"
    facts = []
    if analysis.get("fact"):
        facts.append(analysis["fact"])
    if raw_summary and raw_summary not in facts:
        facts.append(raw_summary)

    payload = {
        "thesis": thesis,
        "facts": facts[:3],
        "sentiment": sentiment
    }
    return json.dumps(payload, ensure_ascii=False)


def get_cached_analysis(source_url: str) -> Optional[Dict[str, Any]]:
    """è·å–ç¼“å­˜çš„ AI åˆ†æç»“æœ"""
    if not source_url:
        return None
    
    try:
        ph = get_placeholder()
        with db_conn() as conn:
            cur = conn.cursor()
            cur.execute(f"""
                SELECT ai_polarity, ai_impacts, ai_opinion, ai_tags, ai_analyzed_at
                FROM raw_articles
                WHERE source_url = {ph} AND ai_analyzed_at IS NOT NULL
            """, (source_url,))
            row = cur.fetchone()
            
            if row:
                # æ”¯æŒ dict å’Œ tuple ä¸¤ç§æ ¼å¼
                if isinstance(row, dict):
                    polarity = row.get("ai_polarity")
                    impacts_str = row.get("ai_impacts")
                    opinion = row.get("ai_opinion") or ""
                    tags_str = row.get("ai_tags")
                else:
                    polarity = row[0]
                    impacts_str = row[1]
                    opinion = row[2] or ""
                    tags_str = row[3]
                
                if polarity:  # ai_polarity å­˜åœ¨è¯´æ˜æœ‰ç¼“å­˜
                    return {
                        "polarity": polarity,
                        "impacts": json.loads(impacts_str) if impacts_str else [],
                        "opinion": opinion,
                        "tags": json.loads(tags_str) if tags_str else [],
                        "cached": True
                    }
    except Exception as e:
        print(f"âš ï¸ è¯»å– AI ç¼“å­˜å¤±è´¥: {e}")
    
    return None


def save_analysis_cache(source_url: str, analysis: Dict[str, Any]) -> None:
    """ä¿å­˜ AI åˆ†æç»“æœåˆ°ç¼“å­˜"""
    if not source_url or not analysis:
        return
    
    try:
        ph = get_placeholder()
        with db_conn() as conn:
            cur = conn.cursor()
            cur.execute(f"""
                UPDATE raw_articles
                SET ai_polarity = {ph},
                    ai_impacts = {ph},
                    ai_opinion = {ph},
                    ai_tags = {ph},
                    ai_analyzed_at = CURRENT_TIMESTAMP
                WHERE source_url = {ph}
            """, (
                analysis.get("polarity", "neutral"),
                json.dumps(analysis.get("impacts", []), ensure_ascii=False),
                analysis.get("opinion", ""),
                json.dumps(analysis.get("tags", []), ensure_ascii=False),
                source_url
            ))
            conn.commit()
            print(f"âœ“ AI åˆ†æå·²ç¼“å­˜: {source_url[:50]}...")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ AI ç¼“å­˜å¤±è´¥: {e}")


def build_intelligence_cards(
    limit: int = 20,
    skip_ai: bool = False,
    category_key: Optional[str] = None
) -> List[Dict[str, Any]]:
    if category_key and category_key != "all":
        data = {category_key: get_articles_for_category(category_key)}
    else:
        data = get_raw_articles_by_category()
    cards: List[Dict[str, Any]] = []
    index = 0

    for category_key, articles in data.items():
        if not isinstance(articles, list):
            continue

        for article in articles[:3]:
            normalized = normalize_article(article, category_key)
            if not normalized:
                continue

            index += 1
            article_id = normalized["id"] or index
            source_url = normalized.get("source_url")

            if skip_ai:
                analysis = {
                    "polarity": "neutral",
                    "impacts": [],
                    "tags": [],
                    "opinion": ""
                }
            else:
                # 1. å…ˆæ£€æŸ¥ç¼“å­˜
                cached = get_cached_analysis(source_url)
                if cached:
                    analysis = cached
                    print(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜: {normalized['title'][:30]}...")
                else:
                    # 2. æ— ç¼“å­˜åˆ™è°ƒç”¨ AI
                    analysis = analyze_article(normalized["title"], normalized["summary"])
                    # 3. ä¿å­˜åˆ°ç¼“å­˜
                    if source_url and analysis.get("polarity"):
                        save_analysis_cache(source_url, analysis)

            tags = analysis.get("tags") or []
            if category_key:
                category_label = get_category_label(category_key)
                if category_label not in tags:
                    tags.append(category_label)

            cards.append({
                "id": article_id,
                "title": normalized["title"],
                "polarity": analysis.get("polarity", "neutral"),
                "fact": analysis.get("fact") or normalized["summary"],
                "impacts": analysis.get("impacts", []),
                "opinion": analysis.get("opinion", ""),
                "tags": tags,
                "source_name": normalized["source_name"],
                "source_url": normalized["source_url"]
            })

            if len(cards) >= limit:
                return cards

    return cards


def find_article_by_id(article_id: int) -> Optional[Dict[str, Any]]:
    data = get_raw_articles_by_category()
    for category_key, articles in data.items():
        if not isinstance(articles, list):
            continue
        for article in articles:
            normalized = normalize_article(article, category_key)
            if not normalized:
                continue
            if normalized["id"] is not None and str(normalized["id"]) == str(article_id):
                return normalized
    return None


def build_daily_briefing(cards: List[Dict[str, Any]]) -> Dict[str, Any]:
    now = datetime.now()
    takeaways = [card.get("fact") for card in cards[:3] if card.get("fact")]
    read_time = f"{max(3, len(cards) * 2)} min read"

    impact_chain = {
        "trigger": cards[0]["title"] if cards else "ä»Šæ—¥æš‚æ— é‡ç‚¹æƒ…æŠ¥",
        "path": [
            f"{impact.get('entity')} {impact.get('trend')}"
            for impact in (cards[0].get("impacts", []) if cards else [])[:3]
        ] or ["ç­‰å¾…æ›´å¤šæ•°æ®"],
        "conclusion": cards[0].get("opinion") if cards else "ç¨åå†è¯•"
    }

    return {
        "date": now.strftime("%Y-%m-%d"),
        "title": "Daily Intelligence Briefing",
        "subtitle": "Today's five signals you should not miss.",
        "read_time": read_time,
        "synthesis": "åŸºäºä»Šæ—¥é«˜ä¿¡å·æƒ…æŠ¥ï¼Œç³»ç»Ÿå·²å®Œæˆç»“æ„åŒ–ç­›é€‰ä¸å½±å“é“¾æ¨æ¼”ã€‚",
        "takeaways": takeaways,
        "top_picks": cards,
        "impact_chain": impact_chain
    }


def fetch_special_data(content: str = "å†…å®¹") -> dict:
    """è°ƒç”¨ Special æ¥å£è·å–åŸå§‹æ•°æ®"""
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {ACCESS_TOKEN}'
    }
    payload = {
        "content": content,
        "chainId": SPECIAL_CHAIN_ID,
        "sync": True
    }
    
    try:
        resp = requests.post(SPECIAL_API_URL, json=payload, headers=headers, timeout=30)
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        print(f"âŒ è·å– Special æ•°æ®å¤±è´¥: {e}")
        return {}


def parse_special_response(response: dict) -> dict:
    """è§£æ Special æ¥å£è¿”å›çš„åµŒå¥—ç»“æ„"""
    try:
        if response.get('res_status_code') != '0':
            return {}
        
        res_content = response.get('res_content', {})
        response_obj = res_content.get('response', {})
        
        if isinstance(response_obj, dict):
            content_field = response_obj.get('content', '')
            if isinstance(content_field, str):
                return json.loads(content_field)
            return content_field
        elif isinstance(response_obj, str):
            return json.loads(response_obj)
    except Exception as e:
        print(f"âŒ è§£æå“åº”å¤±è´¥: {e}")
    
    return {}


def analyze_article(title: str, summary: str) -> dict:
    """
    ä½¿ç”¨å¤–éƒ¨ Analyst Prompt æ¨¡æ¿æ·±åº¦åˆ†ææ–‡ç« 
    å®šä½ï¼šä»â€œæ‘˜è¦å‘˜â€è¿›åŒ–ä¸ºâ€œåˆ†æå¸ˆâ€
    """
    # å°è¯•åŠ è½½å¤–éƒ¨æç¤ºè¯æ¨¡æ¿
    prompt_path = os.path.join(os.path.dirname(__file__), 'prompts', 'analyst_v1.md')
    system_prompt = "You are a Senior Industry Analyst." # å…œåº•
    
    if os.path.exists(prompt_path):
        try:
            with open(prompt_path, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è¯»å–æç¤ºè¯æ–‡ä»¶: {e}")

    user_input = f"TITLE: {title}\nSUMMARY: {summary}"

    try:
        response = ai_client.chat.completions.create(
            model=DEFAULT_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_input}
            ],
            temperature=0.2, # é™ä½éšæœºæ€§ï¼Œæé«˜é€»è¾‘ä¸¥è°¨æ€§
            response_format={ "type": "json_object" } if "gpt-4o" in DEFAULT_MODEL or "qwen" in DEFAULT_MODEL else None
        )
        
        result = response.choices[0].message.content.strip()
        # æ¸…ç† markdown ä»£ç å—
        if result.startswith('```'):
            result = result.split('\n', 1)[1].rsplit('```', 1)[0]
        
        parsed = json.loads(result)
        
        # å…¼å®¹æ€§å¤„ç†ï¼šä¼˜å…ˆä½¿ç”¨ opinionï¼Œè‹¥ä¸å­˜åœ¨åˆ™æŸ¥æ‰¾æ—§å­—æ®µ
        if 'actionable_insight' in parsed:
            parsed['opinion'] = parsed.pop('actionable_insight')
            
        return parsed
    except Exception as e:
        print(f"âš ï¸ AI åˆ†æå¤±è´¥: {e}")
        return {
            "polarity": "neutral",
            "title": title[:15],
            "fact": summary[:40],
            "impacts": [],
            "opinion": "åˆ†æå¼•æ“å“åº”å¼‚å¸¸",
            "tags": [],
            "confidence": "low"
        }


# ============ API è·¯ç”± ============

@app.route('/api/health', methods=['GET'])
def health():
    """å¥åº·æ£€æŸ¥"""
    return success(data={"status": "ok", "model": DEFAULT_MODEL})


@app.route('/api/sync/trigger', methods=['POST'])
def trigger_sync():
    """
    å®šæ—¶ä»»åŠ¡è§¦å‘ç‚¹ - ä¸»åŠ¨åˆ·æ–°æ•°æ®
    å¯ç”± Vercel Cron / Railway Cron / å¤–éƒ¨è°ƒåº¦å™¨è°ƒç”¨
    éœ€è¦ X-Cron-Key å¤´éƒ¨éªŒè¯ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
    """
    cron_secret = os.getenv('CRON_SECRET')
    if cron_secret and request.headers.get('X-Cron-Key') != cron_secret:
        return jsonify({"error": "Unauthorized"}), 401
    
    try:
        data = get_raw_articles_by_category()
        synced_count = sum(len(articles) for articles in data.values())
        return success(
            data={"synced": True, "categories": list(data.keys()), "total_articles": synced_count},
            meta={"timestamp": datetime.now().isoformat()}
        )
    except Exception as e:
        return internal_error(str(e))


@app.route('/api/sync/status', methods=['GET'])
def sync_status():
    """è·å–å„åˆ†ç±»çš„æœ€ååŒæ­¥æ—¶é—´"""
    category_keys = list(get_category_mapping().keys())
    _, last_sync_map = fetch_all_raw_articles_with_metadata(category_keys, limit_per_cat=1)
    
    now = datetime.now()
    status = {}
    for key in category_keys:
        last_sync = last_sync_map.get(key)
        status[key] = {
            "last_sync": last_sync.isoformat() if last_sync else None,
            "is_today": last_sync.date() >= now.date() if last_sync else False
        }
    
    return jsonify({
        "current_time": now.isoformat(),
        "categories": status
    })

@app.route('/api/raw-data', methods=['GET'])
def get_raw_data():
    """è·å–åŸå§‹å…¬ä¼—å·æ•°æ®ï¼ˆæ•°æ®ä¸­å¿ƒç”¨ï¼‰"""
    category = request.args.get('category', 'legal')
    items = get_articles_for_category(category)

    return success(
        data={"category": category, "label": get_category_label(category), "items": items},
        meta={"count": len(items)}
    )


@app.route('/api/intelligence', methods=['GET'])
def get_intelligence():
    """è·å– AI åˆ†æåçš„æƒ…æŠ¥å¡ç‰‡ï¼ˆé¦–é¡µç”¨ï¼‰"""
    limit = int(request.args.get('limit', 20))
    # ç”Ÿäº§ç¯å¢ƒå¯é€šè¿‡ DEFAULT_SKIP_AI=true è·³è¿‡ AI åˆ†æï¼ˆé¿å…å†…ç½‘ API è¶…æ—¶ï¼‰
    default_skip = os.getenv('DEFAULT_SKIP_AI', 'false').lower() == 'true'
    skip_ai = request.args.get('skip_ai', str(default_skip)).lower() == 'true'
    category = request.args.get('category')

    cards = build_intelligence_cards(limit=limit, skip_ai=skip_ai, category_key=category)

    return success(
        data={"cards": cards},
        meta={"count": len(cards)}
    )


@app.route('/api/feed', methods=['GET'])
def get_feed():
    """å…¼å®¹æ—§å‰ç«¯çš„ Feed æ¥å£ï¼ˆè¿”å›æ•°ç»„ï¼‰"""
    limit = int(request.args.get('limit', 20))
    skip_ai = request.args.get('skip_ai', 'false').lower() == 'true'
    category = request.args.get('category')
    cards = build_intelligence_cards(limit=limit, skip_ai=skip_ai, category_key=category)
    return jsonify(cards)


@app.route('/api/article/<int:article_id>', methods=['GET'])
def get_article_detail(article_id: int):
    """è·å–è¯¦æƒ…é¡µæ•°æ®ï¼ˆä¸ /api/intelligence åŒæºï¼‰"""
    skip_ai = request.args.get('skip_ai', 'false').lower() == 'true'
    article = find_article_by_id(article_id)
    if not article:
        return jsonify({"error": "Article not found"}), 404

    if skip_ai:
        analysis = {
            "polarity": "neutral",
            "impacts": [],
            "tags": [],
            "opinion": ""
        }
    else:
        analysis = analyze_article(article["title"], article["summary"])

    tags = analysis.get("tags") or []
    category_label = get_category_label(article["category_key"])
    if category_label and category_label not in tags:
        tags.append(category_label)

    summary_payload = build_summary_payload(analysis, article["summary"])

    return jsonify({
        "id": article["id"],
        "title": article["title"],
        "polarity": analysis.get("polarity", "neutral"),
        "fact": analysis.get("fact") or article["summary"],
        "impacts": analysis.get("impacts", []),
        "opinion": analysis.get("opinion", ""),
        "tags": tags,
        "source_name": article["source_name"],
        "source_url": article["source_url"],
        "content": article["content"],
        "summary": summary_payload,
        "original_url": article["source_url"]
    })


@app.route('/api/categories', methods=['GET'])
def get_categories():
    """è·å–æ‰€æœ‰åˆ†ç±»ï¼ˆå…¼å®¹æ—§ APIï¼‰"""
    return jsonify({
        "categories": [
            {"id": k, "label": v} for k, v in get_category_mapping().items()
        ]
    })


@app.route('/api/tags', methods=['GET'])
def get_tags():
    """è·å–ç»Ÿä¸€æ ‡ç­¾ä½“ç³»"""
    from tags import tag_service, CATEGORY_TAGS
    
    return jsonify({
        "categories": [tag_service.to_dict(tag) for tag in CATEGORY_TAGS],
        "systemTags": [
            {"id": "important", "name": "é‡è¦", "icon": "â­", "color": "#F59E0B", "level": "user"},
            {"id": "follow_up", "name": "å¾…è·Ÿè¿›", "icon": "ğŸ“Œ", "color": "#EF4444", "level": "user"},
            {"id": "archived", "name": "å·²å½’æ¡£", "icon": "ğŸ“", "color": "#94A3B8", "level": "user"},
        ]
    })


@app.route('/api/tags/article/<int:article_id>', methods=['GET'])
def get_article_tags(article_id):
    """è·å–æ–‡ç« çš„æ ‡ç­¾"""
    from tags import tag_service
    
    # TODO: ä»æ•°æ®åº“è·å–æ–‡ç« çš„å®é™…æ ‡ç­¾
    # ç›®å‰è¿”å›ç¤ºä¾‹æ•°æ®
    return jsonify({
        "articleId": article_id,
        "tags": [
            {"id": "cat_legal", "name": "æ³•å¾‹æ³•è§„", "level": "category", "icon": "âš–ï¸", "color": "#6366F1"},
            {"id": "ai_ä¿¡ç”¨ä¿®å¤", "name": "ä¿¡ç”¨ä¿®å¤", "level": "ai", "color": "#71717A"},
        ]
    })


@app.route('/api/reading-record', methods=['POST'])
@validate_json('article_id', 'device_id')
def save_reading_record():
    """ä¿å­˜ç”¨æˆ·é˜…è¯»æ—¶é—´è®°å½•"""
    data = request.get_json()
    
    article_id = data.get('article_id')
    device_id = data.get('device_id')
    duration_seconds = data.get('duration_seconds', 0)
    completed = data.get('completed', False)
    
    try:
        ph = get_placeholder()
        with db_conn() as conn:
            cur = conn.cursor()
            cur.execute(f"""
                INSERT INTO reading_records (article_id, device_id, duration_seconds, completed)
                VALUES ({ph}, {ph}, {ph}, {ph})
            """, (article_id, device_id, duration_seconds, completed))
            conn.commit()
        
        return success()
    except Exception as e:
        print(f"âŒ ä¿å­˜é˜…è¯»è®°å½•å¤±è´¥: {e}")
        return internal_error(str(e))


@app.route('/api/reading-stats', methods=['GET'])
def get_reading_stats():
    """è·å–è®¾å¤‡çš„é˜…è¯»ç»Ÿè®¡"""
    device_id = request.args.get('device_id')
    
    if not device_id:
        return jsonify({"error": "Missing device_id"}), 400
    
    try:
        ph = get_placeholder()
        with db_conn() as conn:
            cur = conn.cursor()
            # è·å–æ€»é˜…è¯»æ—¶é•¿
            cur.execute(f"""
                SELECT 
                    COUNT(*) as total_articles,
                    COALESCE(SUM(duration_seconds), 0) as total_seconds,
                    COUNT(CASE WHEN completed = 1 THEN 1 END) as completed_count
                FROM reading_records
                WHERE device_id = {ph}
            """, (device_id,))
            row = cur.fetchone()
            
            # è·å–æ¯ç¯‡æ–‡ç« çš„é˜…è¯»æ—¶é•¿
            cur.execute(f"""
                SELECT article_id, SUM(duration_seconds) as total_duration
                FROM reading_records
                WHERE device_id = {ph}
                GROUP BY article_id
            """, (device_id,))
            article_times = {r[0]: r[1] for r in cur.fetchall()}
        
        return jsonify({
            "device_id": device_id,
            "total_articles": row[0] if row else 0,
            "total_seconds": row[1] if row else 0,
            "completed_count": row[2] if row else 0,
            "article_reading_times": article_times
        })
    except Exception as e:
        print(f"âŒ è·å–é˜…è¯»ç»Ÿè®¡å¤±è´¥: {e}")
        return jsonify({"error": str(e)}), 500


@app.route('/api/entities', methods=['GET'])
def get_entities():
    """è·å–å®ä½“é›·è¾¾åˆ—è¡¨"""
    entities = entity_service.get_entities()
    result = []
    for entity in entities:
        if hasattr(entity, "model_dump"):
            result.append(entity.model_dump())
        elif hasattr(entity, "dict"):
            result.append(entity.dict())
        else:
            result.append(entity)
    return jsonify(result)


@app.route('/api/entities/toggle/<entity_id>', methods=['POST'])
def toggle_entity_subscription(entity_id: str):
    """è®¢é˜…/å–æ¶ˆè®¢é˜…å®ä½“"""
    is_subscribed = entity_service.toggle_subscription(entity_id)
    return jsonify({"entity_id": entity_id, "is_subscribed": is_subscribed})


@app.route('/api/briefing/daily', methods=['GET'])
def get_daily_briefing():
    """ç”Ÿæˆæ¯æ—¥ç®€æŠ¥ï¼ˆä¸æƒ…æŠ¥å¡ç‰‡åŒæºï¼‰"""
    limit = int(request.args.get('limit', 5))
    skip_ai = request.args.get('skip_ai', 'false').lower() == 'true'
    cards = build_intelligence_cards(limit=limit, skip_ai=skip_ai)
    return jsonify(build_daily_briefing(cards))


# ============ Topics / Fortress API ============

@app.route('/api/topics', methods=['GET', 'POST'])
def handle_topics():
    """å ¡å’ä¸»é¢˜ç®¡ç†"""
    if request.method == 'GET':
        return jsonify(topic_service.get_all_topics())
    elif request.method == 'POST':
        data = request.json
        if not data or 'title' not in data:
            return jsonify({'error': 'Title is required'}), 400
        new_id = topic_service.create_topic(
            data['title'], 
            data.get('description', ''), 
            data.get('channel_key')
        )
        return jsonify({'id': new_id, 'message': 'Topic created'}), 201

@app.route('/api/topics/<int:topic_id>', methods=['GET'])
def get_topic(topic_id):
    """è·å–å•ä¸ªä¸»é¢˜è¯¦æƒ…ï¼ˆå«ç‰ˆæœ¬è½´ä¸è¯æ®é“¾ï¼‰"""
    topic = topic_service.get_topic_detail(topic_id)
    if not topic:
        return jsonify({'error': 'Topic not found'}), 404
    return jsonify(topic)

@app.route('/api/topics/<int:topic_id>/evidence', methods=['POST'])
def add_evidence(topic_id):
    """ä¸ºä¸»é¢˜æ·»åŠ è¯æ®ç –å—"""
    data = request.json
    if not data or 'note' not in data:
        return jsonify({'error': 'Note is required'}), 400
    
    eid = topic_service.add_evidence(
        topic_id,
        data['note'],
        data.get('source_title', ''),
        data.get('source_url', ''),
        data.get('highlight_text')
    )
    return jsonify({'id': eid, 'message': 'Evidence added'}), 201


# ============ å¯åŠ¨ ============

if __name__ == '__main__':
    # å…¨é‡åˆå§‹åŒ– (Supabase / PostgreSQL)
    from database import init_db
    init_db()
    
    print(f"âœ“ AI æ¨¡å‹: {DEFAULT_MODEL}")
    print(f"âœ“ AI æ¥å£: {OPENAI_BASE_URL}")
    print(f"âœ“ Special æ¥å£: {SPECIAL_API_URL}")
    print(f"\nğŸš€ æœ¬åœ°è°ƒè¯•å¯åŠ¨: http://0.0.0.0:8000\n")
    
    app.run(host='0.0.0.0', port=8000, debug=True)
else:
    # ç”Ÿäº§ç¯å¢ƒ (Gunicorn å¯åŠ¨) åˆå§‹åŒ–
    from database import init_db
    init_db()
    
    # å¯åŠ¨æ—¶é¢„çƒ­ï¼šæ£€æŸ¥å¹¶åŒæ­¥ä»Šæ—¥æ•°æ®
    if os.getenv('ENABLE_STARTUP_SYNC', 'true').lower() == 'true':
        try:
            print("ğŸ”„ ç”Ÿäº§ç¯å¢ƒå¯åŠ¨é¢„çƒ­...")
            get_raw_articles_by_category()
            print("âœ… æ•°æ®é¢„çƒ­å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ å¯åŠ¨é¢„çƒ­å¤±è´¥ï¼ˆä¸å½±å“æœåŠ¡ï¼‰: {e}")
