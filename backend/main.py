"""
ContentRSS åç«¯ - AI æƒ…æŠ¥åˆ†ææœåŠ¡
åŸºäº Flask + OpenAI å…¼å®¹ API
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
from openai import OpenAI
import os
import json
import requests
from dotenv import load_dotenv
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from database import init_db, get_db_connection, get_placeholder, is_postgres
import hashlib
from typing import Any, Dict, List, Optional

# æ¡ä»¶å¯¼å…¥ PostgreSQL ä¸“ç”¨æ¨¡å—
if is_postgres():
    from psycopg2.extras import execute_values, RealDictCursor
    from psycopg2.pool import ThreadedConnectionPool

from services.entities import EntityService
from services.tag_service import tag_service
from topics import topic_service

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
OPENAI_BASE_URL = os.getenv('OPENAI_BASE_URL')
DEFAULT_MODEL = os.getenv('DEFAULT_MODEL', 'qwen-max')  # å‡çº§åˆ°æ›´å¼ºæ¨¡å‹
SPECIAL_API_URL = os.getenv('SPECIAL_API_URL')
SPECIAL_CHAIN_ID = int(os.getenv('SPECIAL_CHAIN_ID', '1036'))
ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')

# éªŒè¯é…ç½®
if not OPENAI_API_KEY:
    raise ValueError("ç¼ºå°‘ OPENAI_API_KEY é…ç½®")
if not OPENAI_BASE_URL:
    raise ValueError("ç¼ºå°‘ OPENAI_BASE_URL é…ç½®")

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
ai_client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL
)

# åˆ›å»º Flask åº”ç”¨
app = Flask(__name__)

# ç”Ÿäº§ç¯å¢ƒ CORS é…ç½®
# æ³¨æ„ï¼šstrip() å¤„ç†ç¯å¢ƒå˜é‡ä¸­å¯èƒ½çš„ç©ºæ ¼/æ¢è¡Œ
PROD_ORIGINS = [origin.strip() for origin in os.getenv('ALLOWED_ORIGINS', 'http://localhost:16889,http://localhost:5173').split(',')]
CORS(app, origins=PROD_ORIGINS, supports_credentials=True)

# ç»Ÿä¸€å“åº”å·¥å…·
from utils.response import success, error, not_found, bad_request, internal_error, register_error_handlers, validate_json
register_error_handlers(app)


def get_category_mapping() -> Dict[str, str]:
    """è·å–åˆ†ç±»æ˜ å°„ï¼ˆä»æ•°æ®åº“ï¼Œæ›¿ä»£ç¡¬ç¼–ç ï¼‰"""
    return tag_service.get_category_mapping()


def get_category_label(category_key: str) -> str:
    """è·å–åˆ†ç±»æ˜¾ç¤ºåç§°"""
    mapping = get_category_mapping()
    return mapping.get(category_key, category_key)


# ========== æ•°æ®åº“è¿æ¥ç®¡ç† (SQLite / PostgreSQL åŒæ¨¡å¼) ==========
from database import db_conn, is_postgres
# ================================


entity_service = EntityService()


def _coerce_special_payload(data: Any) -> Dict[str, List[Dict[str, Any]]]:
    if isinstance(data, list):
        return {"insight": data}
    if isinstance(data, dict):
        return {k: v for k, v in data.items() if isinstance(v, list)}
    return {}


def parse_datetime(value: Any) -> Optional[datetime]:
    """è§£ææ—¥æœŸæ—¶é—´ï¼Œå…¼å®¹ datetime å¯¹è±¡å’Œå„ç§å­—ç¬¦ä¸²æ ¼å¼"""
    if value is None:
        return None
    if isinstance(value, datetime):
        return value
    if isinstance(value, str):
        # ç§»é™¤å¯èƒ½çš„å¾®ç§’æˆ–æ—¶åŒºåç¼€ä»¥ä¾¿ç»Ÿä¸€è§£æ
        clean_val = value.split('.')[0].replace('Z', '').replace('T', ' ')
        try:
            return datetime.strptime(clean_val, '%Y-%m-%d %H:%M:%S')
        except ValueError:
            try:
                return datetime.fromisoformat(value)
            except:
                return None
    return None


def get_raw_articles_by_category() -> Dict[str, List[Dict[str, Any]]]:
    """ç»Ÿä¸€è·å–æ‰€æœ‰åˆ†ç±»çš„åŸå§‹æ–‡ç« æ•°æ®ï¼ˆæè‡´ä¼˜åŒ–ï¼šç¼“å­˜å‘½ä¸­ä»… 1 æ¬¡ RTTï¼‰"""
    category_keys = list(get_category_mapping().keys())
    
    # 1. å•æ¬¡æŸ¥è¯¢è·å–æ•°æ® + çŠ¶æ€
    db_data, last_sync_map = fetch_all_raw_articles_with_metadata(category_keys)
    
    # 2. æ£€æŸ¥åŒæ­¥éœ€æ±‚ (æ¯å¤©ä»…éœ€åŒæ­¥ä¸€æ¬¡)
    now = datetime.now()
    needs_sync_keys = []
    
    for key in category_keys:
        last_sync = last_sync_map.get(key)
        # å¦‚æœæ²¡æ•°æ®ï¼Œæˆ–è€…ä¸Šæ¬¡åŒæ­¥ä¸æ˜¯ä»Šå¤©
        if not last_sync or last_sync.date() < now.date():
            needs_sync_keys.append(key)
    
    if needs_sync_keys:
        print(f"ğŸ”„ éœ€è¦åŒæ­¥ ({len(needs_sync_keys)}ä¸ªåˆ†ç±»): {needs_sync_keys}")
        try:
            response = fetch_special_data()
            sync_data = _coerce_special_payload(parse_special_response(response))
            for key in needs_sync_keys:
                items = sync_data.get(key) or []
                if items:
                    persist_raw_items(key, items)
            # åªæœ‰åŒæ­¥å‘ç”Ÿæ—¶æ‰è¿›è¡Œç¬¬äºŒæ¬¡æŸ¥è¯¢
            db_data, _ = fetch_all_raw_articles_with_metadata(category_keys)
        except Exception as e:
            print(f"âš ï¸ åŒæ­¥å¤±è´¥: {e}")

    return db_data

def get_articles_for_category(category_key: str) -> List[Dict[str, Any]]:
    """è·å–å•åˆ†ç±»æ•°æ®ï¼ˆå¤ç”¨æ‰¹é‡é€»è¾‘ä»¥èŠ‚çœè¿æ¥ï¼‰"""
    data = get_raw_articles_by_category()
    return data.get(category_key, [])


def fetch_special_category_items(category_key: str) -> List[Dict[str, Any]]:
    response = fetch_special_data()
    data = _coerce_special_payload(parse_special_response(response))
    return data.get(category_key, []) if data else []


def _row_value(row: Any, key: str) -> Any:
    if isinstance(row, dict):
        return row.get(key)
    try:
        return row[key]
    except Exception:
        return None


def fetch_all_raw_articles_with_metadata(category_keys: List[str], limit_per_cat: int = 40) -> tuple[Dict[str, List[Dict[str, Any]]], Dict[str, datetime]]:
    """æ‰¹é‡ä»æ•°æ®åº“è¯»å–æ–‡ç« å’Œæœ€æ–°çš„åŒæ­¥æ—¶é—´"""
    try:
        if not category_keys:
            return {}, {}
        
        ph = get_placeholder()  # SQLite: ? / PostgreSQL: %s
        placeholders = ", ".join([ph] * len(category_keys))
        
        # SQLite å’Œ PostgreSQL éƒ½æ”¯æŒ WITH å’Œ ROW_NUMBER() çª—å£å‡½æ•°
        query = f"""
            WITH ranked_articles AS (
                SELECT 
                    category_key, 
                    raw_payload,
                    ingested_at,
                    ROW_NUMBER() OVER(PARTITION BY category_key ORDER BY ingested_at DESC) as rank
                FROM raw_articles
                WHERE category_key IN ({placeholders})
            )
            SELECT category_key, raw_payload, ingested_at, rank
            FROM ranked_articles
            WHERE rank <= {ph}
        """
        
        db_data: Dict[str, List[Dict[str, Any]]] = {key: [] for key in category_keys}
        last_sync_map: Dict[str, datetime] = {}
        
        with db_conn() as conn:
            cur = conn.cursor()
            cur.execute(query, (*category_keys, limit_per_cat))
            rows = cur.fetchall()
            
        for row in rows:
            cat = row["category_key"]
            payload = row["raw_payload"]
            ingested_at = row["ingested_at"]
            rank = row["rank"]
            
            # è®°å½•è¯¥åˆ†ç±»æœ€æ–°çš„åŒæ­¥æ—¶é—´ï¼ˆrank=1 çš„å³ä¸ºæœ€æ–°ï¼‰
            if rank == 1:
                last_sync_map[cat] = parse_datetime(ingested_at)
            
            if isinstance(payload, str):
                try:
                    payload = json.loads(payload)
                except:
                    payload = {"raw_payload": payload}
            
            if isinstance(payload, dict):
                # å°† ingested_at æ·»åŠ åˆ°è¿”å›æ•°æ®ä¸­
                payload["ingested_at"] = str(ingested_at) if ingested_at else None
                db_data[cat].append(payload)
                
        return db_data, last_sync_map
    except Exception as e:
        print(f"âš ï¸ æ‰¹é‡è¯»å– raw_articles å¤±è´¥: {e}")
        return {}, {}


def get_all_synced_recently(category_keys: List[str]) -> List[str]:
    """æ‰¹é‡æ£€æŸ¥å“ªäº›åˆ†ç±»ä»Šå¤©å·²ç»åŒæ­¥è¿‡"""
    _, last_sync_map = fetch_all_raw_articles_with_metadata(category_keys, limit_per_cat=1)
    today = datetime.now().date()
    return [k for k, v in last_sync_map.items() if v.date() >= today]


def is_synced_recently(category_key: str) -> bool:
    """å•ä¸ªæ£€æŸ¥"""
    return category_key in get_all_synced_recently([category_key])



def persist_raw_items(category_key: str, items: List[Dict[str, Any]]) -> None:
    """æ‰¹é‡ä¿å­˜æ–‡ç« ï¼Œæ”¯æŒ SQLite å’Œ PostgreSQL"""
    if not items:
        return
    try:
        data_to_insert = []
        for item in items:
            normalized = normalize_article(item, category_key)
            if not normalized:
                continue
            payload = json.dumps(item, ensure_ascii=False)
            data_to_insert.append((
                normalized["source_name"],
                normalized["source_url"],
                normalized["title"],
                normalized["summary"],
                normalized["content"],
                category_key,
                payload,
                None # published_at
            ))

        if not data_to_insert:
            return

        with db_conn() as conn:
            cur = conn.cursor()
            if is_postgres():
                # PostgreSQL: ä½¿ç”¨ execute_values æ‰¹é‡æ’å…¥
                query = """
                    INSERT INTO raw_articles 
                    (source_name, source_url, title, summary, content, category_key, raw_payload, published_at) 
                    VALUES %s
                    ON CONFLICT (source_url) DO UPDATE SET 
                    source_name = EXCLUDED.source_name, 
                    title = EXCLUDED.title, 
                    summary = EXCLUDED.summary, 
                    content = EXCLUDED.content, 
                    category_key = EXCLUDED.category_key, 
                    raw_payload = EXCLUDED.raw_payload, 
                    published_at = EXCLUDED.published_at, 
                    ingested_at = CURRENT_TIMESTAMP
                """
                execute_values(cur, query, data_to_insert)
            else:
                # SQLite: é€æ¡æ’å…¥ (INSERT OR REPLACE)
                query = """
                    INSERT OR REPLACE INTO raw_articles 
                    (source_name, source_url, title, summary, content, category_key, raw_payload, published_at) 
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """
                cur.executemany(query, data_to_insert)
            conn.commit()
            print(f"âœ… å·²å­˜å…¥ {len(data_to_insert)} æ¡æ•°æ®åˆ°åˆ†ç±» {category_key}")
    except Exception as e:
        print(f"âš ï¸ æ‰¹é‡ä¿å­˜ raw_articles å¤±è´¥: {e}")


def safe_json(value: Any) -> Dict[str, Any]:
    if isinstance(value, dict):
        return value
    if isinstance(value, str):
        try:
            return json.loads(value)
        except Exception:
            return {}
    return {}


def normalize_article(article: Dict[str, Any], category_key: str) -> Optional[Dict[str, Any]]:
    if not isinstance(article, dict):
        return None
        
    fields = article.get("fields", article)
    raw_info = fields.get("æ–‡ç« ä¿¡æ¯") or fields.get("article_info") or fields.get("info")
    info = safe_json(raw_info) if raw_info else {}

    title = info.get("æ–‡ç« æ ‡é¢˜") or fields.get("æ–‡ç« æ ‡é¢˜-mossç”¨") or fields.get("title") or ""
    summary = info.get("æ‘˜è¦") or fields.get("æ‘˜è¦") or fields.get("summary") or ""
    content = fields.get("æ–‡ç« å†…å®¹") or fields.get("content") or summary or ""
    source_name = info.get("ä½œè€…åç§°") or fields.get("source_name") or fields.get("source") or ""
    source_url = info.get("æ–‡ç« URL") or fields.get("source_url") or fields.get("url")
    raw_id = fields.get("è‡ªå¢ID") or fields.get("id") or fields.get("article_id")

    if not title:
        return None

    try:
        article_id = int(raw_id) if raw_id is not None else None
    except Exception:
        article_id = None

    if article_id is None:
        id_source = f"{title}|{source_url or ''}"
        article_id = int(hashlib.md5(id_source.encode("utf-8")).hexdigest()[:8], 16)

    # æå– ingested_atï¼ˆæ¥è‡ªæ•°æ®åº“æŸ¥è¯¢ç»“æœï¼‰
    ingested_at = fields.get("ingested_at") or article.get("ingested_at")

    return {
        "id": article_id,
        "title": title,
        "summary": summary,
        "content": content,
        "source_name": source_name,
        "source_url": source_url,
        "category_key": category_key,
        "ingested_at": ingested_at,
    }


def build_summary_payload(analysis: Dict[str, Any], raw_summary: str) -> str:
    polarity = analysis.get("polarity", "neutral")
    sentiment = "neutral"
    if polarity == "positive":
        sentiment = "bullish"
    elif polarity == "negative":
        sentiment = "bearish"

    thesis = analysis.get("opinion") or analysis.get("fact") or raw_summary or "æš‚æ— æ‘˜è¦"
    facts = []
    if analysis.get("fact"):
        facts.append(analysis["fact"])
    if raw_summary and raw_summary not in facts:
        facts.append(raw_summary)

    payload = {
        "thesis": thesis,
        "facts": facts[:3],
        "sentiment": sentiment
    }
    return json.dumps(payload, ensure_ascii=False)


def get_cached_analysis(source_url: str) -> Optional[Dict[str, Any]]:
    """è·å–ç¼“å­˜çš„ AI åˆ†æç»“æœ"""
    if not source_url:
        return None
    
    try:
        ph = get_placeholder()
        with db_conn() as conn:
            cur = conn.cursor()
            cur.execute(f"""
                SELECT ai_polarity, ai_impacts, ai_opinion, ai_tags, ai_analyzed_at
                FROM raw_articles
                WHERE source_url = {ph} AND ai_analyzed_at IS NOT NULL
            """, (source_url,))
            row = cur.fetchone()
            
            if row:
                # æ”¯æŒ dict å’Œ tuple ä¸¤ç§æ ¼å¼
                if isinstance(row, dict):
                    polarity = row.get("ai_polarity")
                    impacts_str = row.get("ai_impacts")
                    opinion = row.get("ai_opinion") or ""
                    tags_str = row.get("ai_tags")
                else:
                    polarity = row[0]
                    impacts_str = row[1]
                    opinion = row[2] or ""
                    tags_str = row[3]
                
                if polarity:  # ai_polarity å­˜åœ¨è¯´æ˜æœ‰ç¼“å­˜
                    return {
                        "polarity": polarity,
                        "impacts": json.loads(impacts_str) if impacts_str else [],
                        "opinion": opinion,
                        "tags": json.loads(tags_str) if tags_str else [],
                        "cached": True
                    }
    except Exception as e:
        print(f"âš ï¸ è¯»å– AI ç¼“å­˜å¤±è´¥: {e}")
    
    return None


def save_analysis_cache(source_url: str, analysis: Dict[str, Any]) -> None:
    """ä¿å­˜ AI åˆ†æç»“æœåˆ°ç¼“å­˜"""
    if not source_url or not analysis:
        return
    
    try:
        ph = get_placeholder()
        with db_conn() as conn:
            cur = conn.cursor()
            cur.execute(f"""
                UPDATE raw_articles
                SET ai_polarity = {ph},
                    ai_impacts = {ph},
                    ai_opinion = {ph},
                    ai_tags = {ph},
                    ai_analyzed_at = CURRENT_TIMESTAMP
                WHERE source_url = {ph}
            """, (
                analysis.get("polarity", "neutral"),
                json.dumps(analysis.get("impacts", []), ensure_ascii=False),
                analysis.get("opinion", ""),
                json.dumps(analysis.get("tags", []), ensure_ascii=False),
                source_url
            ))
            conn.commit()
            print(f"âœ“ AI åˆ†æå·²ç¼“å­˜: {source_url[:50]}...")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ AI ç¼“å­˜å¤±è´¥: {e}")


def build_intelligence_cards(
    limit: int = 20,
    skip_ai: bool = False,
    category_key: Optional[str] = None
) -> List[Dict[str, Any]]:
    if category_key and category_key != "all":
        data = {category_key: get_articles_for_category(category_key)}
    else:
        data = get_raw_articles_by_category()
    
    # 1. æ”¶é›†å¾…å¤„ç†çš„æ–‡ç« åˆ—è¡¨
    pending_tasks = []
    for cat_key, articles in data.items():
        if not isinstance(articles, list):
            continue
        for article in articles[:3]:
            normalized = normalize_article(article, cat_key)
            if normalized:
                pending_tasks.append((normalized, cat_key))

    # é™åˆ¶æ€»é‡
    pending_tasks = pending_tasks[:limit]
    
    # 2. å®šä¹‰å¤„ç†å•å…ƒ
    def process_one(task):
        normalized, cat_key = task
        source_url = normalized.get("source_url")
        
        if skip_ai:
            analysis = {
                "polarity": "neutral",
                "impacts": [],
                "tags": [],
                "opinion": ""
            }
        else:
            # 1. å…ˆæ£€æŸ¥ç¼“å­˜
            cached = get_cached_analysis(source_url)
            if cached:
                analysis = cached
                print(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜: {normalized['title'][:30]}...")
            else:
                # 2. æ— ç¼“å­˜åˆ™è°ƒç”¨ AI (è¿™æ˜¯æœ€è€—æ—¶çš„æ­¥éª¤)
                print(f"ğŸ¤– AI åˆ†æä¸­: {normalized['title'][:30]}...")
                analysis = analyze_article(normalized["title"], normalized["summary"])
                # 3. ä¿å­˜åˆ°ç¼“å­˜
                if source_url and analysis.get("polarity"):
                    save_analysis_cache(source_url, analysis)
        
        tags = analysis.get("tags") or []
        if cat_key:
            category_label = get_category_label(cat_key)
            if category_label not in tags:
                tags.append(category_label)
        
        return {
            "id": normalized["id"],
            "title": normalized["title"],
            "polarity": analysis.get("polarity", "neutral"),
            "fact": analysis.get("fact") or normalized["summary"],
            "impacts": analysis.get("impacts", []),
            "opinion": analysis.get("opinion", ""),
            "tags": tags,
            "source_name": normalized["source_name"],
            "source_url": normalized["source_url"],
            "ingested_at": normalized.get("ingested_at")
        }

    # 3. å¹¶å‘æ‰§è¡Œ
    cards = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_article = {executor.submit(process_one, task): task for task in pending_tasks}
        for future in as_completed(future_to_article):
            try:
                card = future.result()
                if card:
                    cards.append(card)
            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ–‡ç« å¤±è´¥: {e}")

    # ä¿æŒä¸€å®šçš„æ’åºé¡ºåºï¼ˆå¯é€‰ï¼Œç›®å‰å¤šçº¿ç¨‹è¿”å›é¡ºåºæ˜¯éšæœºçš„ï¼‰
    return cards

def find_article_by_id(article_id: int) -> Optional[Dict[str, Any]]:
    data = get_raw_articles_by_category()
    for category_key, articles in data.items():
        if not isinstance(articles, list):
            continue
        for article in articles:
            normalized = normalize_article(article, category_key)
            if not normalized:
                continue
            if normalized["id"] is not None and str(normalized["id"]) == str(article_id):
                return normalized
    return None


def build_daily_briefing(cards: List[Dict[str, Any]], persona: str = "SPECIALIST") -> Dict[str, Any]:
    now = datetime.now()
    takeaways = [card.get("fact") for card in cards[:3] if card.get("fact")]
    read_time = f"{max(3, len(cards) * 2)} min read"

    persona_configs = {
        "VISIONARY": {
            "title": "Visionary Hub",
            "subtitle": "è¿½è¸ªé¢ è¦†æ€§æŠ€æœ¯çš„æŒ‡æ•°çº§å¢é•¿ä¿¡å·",
            "synthesis": "åŸºäº [æŠ€æœ¯è¿œè§è€…] æ¨¡å¼ï¼Œç³»ç»Ÿå·²å¯¹å®éªŒå®¤çº§åˆ«çš„ç‰©ç†çªç ´ä¸é•¿å‘¨æœŸæŠ€æœ¯å€ºåŠ¡è¿›è¡Œäº†å…³è”åˆ†æã€‚"
        },
        "INVESTOR": {
            "title": "Alpha Pursuit",
            "subtitle": "é”å®šèµ„æœ¬å¸‚åœºçš„éå¯¹ç§°è·åˆ©çª—å£",
            "synthesis": "åŸºäº [ä»·å€¼æŠ•èµ„è€…] æ¨¡å¼ï¼Œå·²å‰”é™¤çŸ­æœŸå™ªéŸ³ï¼Œé‡ç‚¹æ­ç¤ºè´¢åŠ¡åŸºæœ¬é¢ä¸å®è§‚æ”¿ç­–çš„å…±é¸£èŠ‚ç‚¹ã€‚"
        },
        "SPECIALIST": {
            "title": "Specialist Brain",
            "subtitle": "æ‹†è§£äº§å“æ¼”è¿›ä¸æè‡´ä½“éªŒçš„å¾®è§‚ç»†èŠ‚",
            "synthesis": "åŸºäº [äº§å“ä¸“å®¶] æ¨¡å¼ï¼Œå·²å¯¹ 15 ä¸ªç«å¯¹åŠŸèƒ½ç‚¹è¿›è¡Œäº†é€†å‘æ‹†è§£ï¼Œèšç„¦å¢é•¿é»‘å®¢è·¯å¾„ã€‚"
        },
        "FOUNDER": {
            "title": "Founder's Choice",
            "subtitle": "è·å–é©±åŠ¨ç»„ç»‡è¿›åŒ–ä¸èµ„æºæ•´åˆçš„é¡¶çº§æƒ…æŠ¥",
            "synthesis": "åŸºäº [åˆ›ä¸šè€…] æ¨¡å¼ï¼Œæƒ…æŠ¥å·²æŒ‰â€˜ç”Ÿå­˜/æ‰©å¼ /é˜²å®ˆâ€™ä¸‰ä¸ªç»´åº¦é‡æ–°æ’å¸ƒï¼Œé‡ç‚¹å…³æ³¨èµ„æœ¬æ•ˆç‡ã€‚"
        }
    }
    
    config = persona_configs.get(persona, persona_configs["SPECIALIST"])

    impact_chain = {
        "trigger": cards[0]["title"] if cards else "ä»Šæ—¥æš‚æ— é‡ç‚¹æƒ…æŠ¥",
        "path": [
            f"{impact.get('entity')} {impact.get('trend')}"
            for impact in (cards[0].get("impacts", []) if cards else [])[:3]
        ] or ["ç­‰å¾…æ›´å¤šæ•°æ®"],
        "conclusion": cards[0].get("opinion") if cards else "ç¨åå†è¯•"
    }

    return {
        "date": now.strftime("%Y-%m-%d"),
        "title": config["title"],
        "subtitle": config["subtitle"],
        "read_time": read_time,
        "synthesis": config["synthesis"],
        "takeaways": takeaways,
        "top_picks": cards,
        "impact_chain": impact_chain
    }


# ============ Daily Briefing å“åº”çº§ç¼“å­˜ ============

def get_cached_daily_briefing(persona: str = "SPECIALIST") -> Optional[Dict[str, Any]]:
    """è·å–å½“å¤©ç¼“å­˜çš„ daily briefing å“åº”ï¼ˆæ¯å¤©æ¯ä¸ªè§’è‰²ç”Ÿæˆä¸€æ¬¡ï¼‰"""
    today = datetime.now().strftime("%Y-%m-%d")
    cache_key = f"daily_briefing_{persona}_{today}"

    try:
        ph = get_placeholder()
        with db_conn() as conn:
            cur = conn.cursor()
            # ä½¿ç”¨ api_cache è¡¨å­˜å‚¨å“åº”çº§ç¼“å­˜
            cur.execute(f"""
                SELECT response_data, created_at
                FROM api_cache
                WHERE cache_key = {ph} AND DATE(created_at) = DATE(CURRENT_TIMESTAMP)
            """, (cache_key,))
            row = cur.fetchone()

            if row:
                if isinstance(row, dict):
                    data = row.get("response_data")
                else:
                    data = row[0]

                if data:
                    print(f"âš¡ Daily Briefing å‘½ä¸­ç¼“å­˜: {today}")
                    return json.loads(data) if isinstance(data, str) else data
    except Exception as e:
        print(f"âš ï¸ è¯»å– Daily Briefing ç¼“å­˜å¤±è´¥: {e}")

    return None


def save_daily_briefing_cache(briefing_data: Dict[str, Any], persona: str = "SPECIALIST") -> None:
    """ä¿å­˜ daily briefing å“åº”åˆ°ç¼“å­˜"""
    today = datetime.now().strftime("%Y-%m-%d")
    cache_key = f"daily_briefing_{persona}_{today}"

    try:
        ph = get_placeholder()
        with db_conn() as conn:
            cur = conn.cursor()

            # ç¡®ä¿ api_cache è¡¨å­˜åœ¨
            if is_postgres():
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS api_cache (
                        id SERIAL PRIMARY KEY,
                        cache_key VARCHAR(255) UNIQUE NOT NULL,
                        response_data JSONB,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
            else:
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS api_cache (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        cache_key TEXT UNIQUE NOT NULL,
                        response_data TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)

            # æ’å…¥æˆ–æ›´æ–°ç¼“å­˜
            response_json = json.dumps(briefing_data, ensure_ascii=False)
            if is_postgres():
                cur.execute(f"""
                    INSERT INTO api_cache (cache_key, response_data, created_at)
                    VALUES ({ph}, {ph}, CURRENT_TIMESTAMP)
                    ON CONFLICT (cache_key) DO UPDATE SET
                        response_data = EXCLUDED.response_data,
                        created_at = CURRENT_TIMESTAMP
                """, (cache_key, response_json))
            else:
                cur.execute(f"""
                    INSERT OR REPLACE INTO api_cache (cache_key, response_data, created_at)
                    VALUES ({ph}, {ph}, CURRENT_TIMESTAMP)
                """, (cache_key, response_json))

            conn.commit()
            print(f"ğŸ’¾ Daily Briefing å·²ç¼“å­˜: {today}")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ Daily Briefing ç¼“å­˜å¤±è´¥: {e}")


def fetch_special_data(content: str = "å†…å®¹") -> dict:
    """è°ƒç”¨ Special æ¥å£è·å–åŸå§‹æ•°æ®"""
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {ACCESS_TOKEN}'
    }
    payload = {
        "content": content,
        "chainId": SPECIAL_CHAIN_ID,
        "sync": True
    }
    
    try:
        resp = requests.post(SPECIAL_API_URL, json=payload, headers=headers, timeout=30)
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        print(f"âŒ è·å– Special æ•°æ®å¤±è´¥: {e}")
        return {}


def parse_special_response(response: dict) -> dict:
    """è§£æ Special æ¥å£è¿”å›çš„åµŒå¥—ç»“æ„"""
    try:
        if response.get('res_status_code') != '0':
            return {}
        
        res_content = response.get('res_content', {})
        response_obj = res_content.get('response', {})
        
        if isinstance(response_obj, dict):
            content_field = response_obj.get('content', '')
            if isinstance(content_field, str):
                return json.loads(content_field)
            return content_field
        elif isinstance(response_obj, str):
            return json.loads(response_obj)
    except Exception as e:
        print(f"âŒ è§£æå“åº”å¤±è´¥: {e}")
    
    return {}


def analyze_article(title: str, summary: str) -> dict:
    """
    ä½¿ç”¨å¤–éƒ¨ Analyst Prompt æ¨¡æ¿æ·±åº¦åˆ†ææ–‡ç« 
    å®šä½ï¼šä»â€œæ‘˜è¦å‘˜â€è¿›åŒ–ä¸ºâ€œåˆ†æå¸ˆâ€
    """
    # å°è¯•åŠ è½½å¤–éƒ¨æç¤ºè¯æ¨¡æ¿
    prompt_path = os.path.join(os.path.dirname(__file__), 'prompts', 'analyst_v1.md')
    system_prompt = "You are a Senior Industry Analyst." # å…œåº•
    
    if os.path.exists(prompt_path):
        try:
            with open(prompt_path, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è¯»å–æç¤ºè¯æ–‡ä»¶: {e}")

    user_input = f"TITLE: {title}\nSUMMARY: {summary}"

    try:
        response = ai_client.chat.completions.create(
            model=DEFAULT_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_input}
            ],
            temperature=0.2, # é™ä½éšæœºæ€§ï¼Œæé«˜é€»è¾‘ä¸¥è°¨æ€§
            response_format={ "type": "json_object" } if "gpt-4o" in DEFAULT_MODEL or "qwen" in DEFAULT_MODEL else None
        )
        
        result = response.choices[0].message.content.strip()
        # æ¸…ç† markdown ä»£ç å—
        if result.startswith('```'):
            result = result.split('\n', 1)[1].rsplit('```', 1)[0]
        
        parsed = json.loads(result)
        
        # å…¼å®¹æ€§å¤„ç†ï¼šä¼˜å…ˆä½¿ç”¨ opinionï¼Œè‹¥ä¸å­˜åœ¨åˆ™æŸ¥æ‰¾æ—§å­—æ®µ
        if 'actionable_insight' in parsed:
            parsed['opinion'] = parsed.pop('actionable_insight')
            
        return parsed
    except Exception as e:
        print(f"âš ï¸ AI åˆ†æå¤±è´¥: {e}")
        return {
            "polarity": "neutral",
            "title": title[:15],
            "fact": summary[:40],
            "impacts": [],
            "opinion": "åˆ†æå¼•æ“å“åº”å¼‚å¸¸",
            "tags": [],
            "confidence": "low"
        }


# ============ API è·¯ç”± ============

@app.route('/api/health', methods=['GET'])
def health():
    """å¥åº·æ£€æŸ¥"""
    return success(data={"status": "ok", "model": DEFAULT_MODEL})


@app.route('/api/sync/trigger', methods=['POST'])
def trigger_sync():
    """
    å®šæ—¶ä»»åŠ¡è§¦å‘ç‚¹ - ä¸»åŠ¨åˆ·æ–°æ•°æ®
    å¯ç”± Vercel Cron / Railway Cron / å¤–éƒ¨è°ƒåº¦å™¨è°ƒç”¨
    éœ€è¦ X-Cron-Key å¤´éƒ¨éªŒè¯ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
    """
    cron_secret = os.getenv('CRON_SECRET')
    if cron_secret and request.headers.get('X-Cron-Key') != cron_secret:
        return jsonify({"error": "Unauthorized"}), 401
    
    try:
        data = get_raw_articles_by_category()
        synced_count = sum(len(articles) for articles in data.values())
        return success(
            data={"synced": True, "categories": list(data.keys()), "total_articles": synced_count},
            meta={"timestamp": datetime.now().isoformat()}
        )
    except Exception as e:
        return internal_error(str(e))


@app.route('/api/sync/status', methods=['GET'])
def sync_status():
    """è·å–å„åˆ†ç±»çš„æœ€ååŒæ­¥æ—¶é—´"""
    category_keys = list(get_category_mapping().keys())
    _, last_sync_map = fetch_all_raw_articles_with_metadata(category_keys, limit_per_cat=1)
    
    now = datetime.now()
    status = {}
    for key in category_keys:
        last_sync = last_sync_map.get(key)
        status[key] = {
            "last_sync": last_sync.isoformat() if last_sync else None,
            "is_today": last_sync.date() >= now.date() if last_sync else False
        }
    
    return jsonify({
        "current_time": now.isoformat(),
        "categories": status
    })

@app.route('/api/raw-data', methods=['GET'])
def get_raw_data():
    """è·å–åŸå§‹å…¬ä¼—å·æ•°æ®ï¼ˆæ•°æ®ä¸­å¿ƒç”¨ï¼‰

    Query Parameters:
        category: åˆ†ç±» key (ai, digital, legal, finance, vc)
        date: å¯é€‰ï¼Œæ ¼å¼ YYYY-MM-DDï¼Œç­›é€‰æŒ‡å®šæ—¥æœŸçš„æ•°æ®

    Returns:
        - ä¸ä¼  date: è¿”å›æ•°æ®åº“ä¸­è¯¥åˆ†ç±»çš„æœ€æ–°æ•°æ®ï¼ˆæœ€å¤š40æ¡ï¼‰
        - ä¼  date: è¿”å›æŒ‡å®šæ—¥æœŸå…¥åº“çš„æ•°æ®
    """
    category = request.args.get('category', 'legal')
    date_str = request.args.get('date')  # å¯é€‰ï¼šYYYY-MM-DD

    items = get_articles_for_category(category)

    # å¦‚æœæŒ‡å®šäº†æ—¥æœŸï¼ŒæŒ‰ ingested_at ç­›é€‰
    if date_str:
        try:
            target_date = date_str  # ç›´æ¥ç”¨å­—ç¬¦ä¸²æ¯”è¾ƒ YYYY-MM-DD å‰ç¼€
            items = [
                item for item in items
                if item.get('ingested_at', '').startswith(target_date)
            ]
        except Exception as e:
            return error(
                code="INVALID_DATE",
                message=f"æ—¥æœŸæ ¼å¼é”™è¯¯: {date_str}ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD",
                status_code=400
            )

    return success(
        data={
            "category": category,
            "label": get_category_label(category),
            "items": items,
            "date_filter": date_str  # è¿”å›ç­›é€‰æ¡ä»¶ï¼Œä¾¿äºå‰ç«¯ç¡®è®¤
        },
        meta={
            "count": len(items),
            "filtered_by_date": date_str is not None
        }
    )


@app.route('/api/intelligence', methods=['GET'])
def get_intelligence():
    """è·å– AI åˆ†æåçš„æƒ…æŠ¥å¡ç‰‡ï¼ˆé¦–é¡µç”¨ï¼‰"""
    limit = int(request.args.get('limit', 20))
    # ç”Ÿäº§ç¯å¢ƒå¯é€šè¿‡ DEFAULT_SKIP_AI=true è·³è¿‡ AI åˆ†æï¼ˆé¿å…å†…ç½‘ API è¶…æ—¶ï¼‰
    default_skip = os.getenv('DEFAULT_SKIP_AI', 'false').lower() == 'true'
    skip_ai = request.args.get('skip_ai', str(default_skip)).lower() == 'true'
    category = request.args.get('category')

    cards = build_intelligence_cards(limit=limit, skip_ai=skip_ai, category_key=category)

    return success(
        data={"cards": cards},
        meta={"count": len(cards)}
    )


@app.route('/api/feed', methods=['GET'])
def get_feed():
    """å…¼å®¹æ—§å‰ç«¯çš„ Feed æ¥å£ï¼ˆè¿”å›æ•°ç»„ï¼‰"""
    limit = int(request.args.get('limit', 20))
    skip_ai = request.args.get('skip_ai', 'false').lower() == 'true'
    category = request.args.get('category')
    cards = build_intelligence_cards(limit=limit, skip_ai=skip_ai, category_key=category)
    return jsonify(cards)


@app.route('/api/article/<int:article_id>', methods=['GET'])
def get_article_detail(article_id: int):
    """è·å–è¯¦æƒ…é¡µæ•°æ®ï¼ˆä¸ /api/intelligence åŒæºï¼‰"""
    skip_ai = request.args.get('skip_ai', 'false').lower() == 'true'
    article = find_article_by_id(article_id)
    if not article:
        return jsonify({"error": "Article not found"}), 404

    if skip_ai:
        analysis = {
            "polarity": "neutral",
            "impacts": [],
            "tags": [],
            "opinion": ""
        }
    else:
        analysis = analyze_article(article["title"], article["summary"])

    tags = analysis.get("tags") or []
    category_label = get_category_label(article["category_key"])
    if category_label and category_label not in tags:
        tags.append(category_label)

    summary_payload = build_summary_payload(analysis, article["summary"])

    return jsonify({
        "id": article["id"],
        "title": article["title"],
        "polarity": analysis.get("polarity", "neutral"),
        "fact": analysis.get("fact") or article["summary"],
        "impacts": analysis.get("impacts", []),
        "opinion": analysis.get("opinion", ""),
        "tags": tags,
        "source_name": article["source_name"],
        "source_url": article["source_url"],
        "content": article["content"],
        "summary": summary_payload,
        "original_url": article["source_url"]
    })


@app.route('/api/categories', methods=['GET'])
def get_categories():
    """è·å–æ‰€æœ‰åˆ†ç±»ï¼ˆå…¼å®¹æ—§ APIï¼‰"""
    return jsonify({
        "categories": [
            {"id": k, "label": v} for k, v in get_category_mapping().items()
        ]
    })


@app.route('/api/tags', methods=['GET'])
def get_tags():
    """è·å–ç»Ÿä¸€æ ‡ç­¾ä½“ç³»"""
    from tags import tag_service, CATEGORY_TAGS
    
    return jsonify({
        "categories": [tag_service.to_dict(tag) for tag in CATEGORY_TAGS],
        "systemTags": [
            {"id": "important", "name": "é‡è¦", "icon": "â­", "color": "#F59E0B", "level": "user"},
            {"id": "follow_up", "name": "å¾…è·Ÿè¿›", "icon": "ğŸ“Œ", "color": "#EF4444", "level": "user"},
            {"id": "archived", "name": "å·²å½’æ¡£", "icon": "ğŸ“", "color": "#94A3B8", "level": "user"},
        ]
    })


@app.route('/api/tags/article/<int:article_id>', methods=['GET'])
def get_article_tags(article_id):
    """è·å–æ–‡ç« çš„æ ‡ç­¾"""
    from tags import tag_service
    
    # TODO: ä»æ•°æ®åº“è·å–æ–‡ç« çš„å®é™…æ ‡ç­¾
    # ç›®å‰è¿”å›ç¤ºä¾‹æ•°æ®
    return jsonify({
        "articleId": article_id,
        "tags": [
            {"id": "cat_legal", "name": "æ³•å¾‹æ³•è§„", "level": "category", "icon": "âš–ï¸", "color": "#6366F1"},
            {"id": "ai_ä¿¡ç”¨ä¿®å¤", "name": "ä¿¡ç”¨ä¿®å¤", "level": "ai", "color": "#71717A"},
        ]
    })


@app.route('/api/reading-record', methods=['POST'])
@validate_json('article_id', 'device_id')
def save_reading_record():
    """ä¿å­˜ç”¨æˆ·é˜…è¯»æ—¶é—´è®°å½•"""
    data = request.get_json()
    
    article_id = data.get('article_id')
    device_id = data.get('device_id')
    duration_seconds = data.get('duration_seconds', 0)
    completed = data.get('completed', False)
    
    try:
        ph = get_placeholder()
        with db_conn() as conn:
            cur = conn.cursor()
            cur.execute(f"""
                INSERT INTO reading_records (article_id, device_id, duration_seconds, completed)
                VALUES ({ph}, {ph}, {ph}, {ph})
            """, (article_id, device_id, duration_seconds, completed))
            conn.commit()
        
        return success()
    except Exception as e:
        print(f"âŒ ä¿å­˜é˜…è¯»è®°å½•å¤±è´¥: {e}")
        return internal_error(str(e))


@app.route('/api/reading-stats', methods=['GET'])
def get_reading_stats():
    """è·å–è®¾å¤‡çš„é˜…è¯»ç»Ÿè®¡"""
    device_id = request.args.get('device_id')
    
    if not device_id:
        return jsonify({"error": "Missing device_id"}), 400
    
    try:
        ph = get_placeholder()
        with db_conn() as conn:
            cur = conn.cursor()
            # è·å–æ€»é˜…è¯»æ—¶é•¿
            cur.execute(f"""
                SELECT 
                    COUNT(*) as total_articles,
                    COALESCE(SUM(duration_seconds), 0) as total_seconds,
                    COUNT(CASE WHEN completed = 1 THEN 1 END) as completed_count
                FROM reading_records
                WHERE device_id = {ph}
            """, (device_id,))
            row = cur.fetchone()
            
            # è·å–æ¯ç¯‡æ–‡ç« çš„é˜…è¯»æ—¶é•¿
            cur.execute(f"""
                SELECT article_id, SUM(duration_seconds) as total_duration
                FROM reading_records
                WHERE device_id = {ph}
                GROUP BY article_id
            """, (device_id,))
            article_times = {r[0]: r[1] for r in cur.fetchall()}
        
        return jsonify({
            "device_id": device_id,
            "total_articles": row[0] if row else 0,
            "total_seconds": row[1] if row else 0,
            "completed_count": row[2] if row else 0,
            "article_reading_times": article_times
        })
    except Exception as e:
        print(f"âŒ è·å–é˜…è¯»ç»Ÿè®¡å¤±è´¥: {e}")
        return jsonify({"error": str(e)}), 500


@app.route('/api/entities', methods=['GET'])
def get_entities():
    """è·å–å®ä½“é›·è¾¾åˆ—è¡¨"""
    entities = entity_service.get_entities()
    result = []
    for entity in entities:
        if hasattr(entity, "model_dump"):
            result.append(entity.model_dump())
        elif hasattr(entity, "dict"):
            result.append(entity.dict())
        else:
            result.append(entity)
    return jsonify(result)


@app.route('/api/entities/toggle/<entity_id>', methods=['POST'])
def toggle_entity_subscription(entity_id: str):
    """è®¢é˜…/å–æ¶ˆè®¢é˜…å®ä½“"""
    is_subscribed = entity_service.toggle_subscription(entity_id)
    return jsonify({"entity_id": entity_id, "is_subscribed": is_subscribed})


@app.route('/api/briefing/daily', methods=['GET'])
def get_daily_briefing():
    """ç”Ÿæˆæ¯æ—¥ç®€æŠ¥ï¼ˆå¸¦å“åº”çº§ç¼“å­˜ï¼Œæ”¯æŒ Persona å·®å¼‚åŒ–ï¼Œæ¯å¤©æ¯è§’è‰²åªç”Ÿæˆä¸€æ¬¡ï¼‰"""
    # 1. æ£€æŸ¥è§’è‰²ä¸å¼ºåˆ¶åˆ·æ–°
    persona = request.args.get('persona', 'SPECIALIST')
    force_refresh = request.args.get('refresh', 'false').lower() == 'true'

    # 2. å°è¯•è¯»å–ç¼“å­˜ï¼ˆé™¤éå¼ºåˆ¶åˆ·æ–°ï¼‰
    if not force_refresh:
        cached = get_cached_daily_briefing(persona=persona)
        if cached:
            return jsonify(cached)

    # 3. æ— ç¼“å­˜æˆ–å¼ºåˆ¶åˆ·æ–°ï¼Œé‡æ–°ç”Ÿæˆ
    limit = int(request.args.get('limit', 5))
    skip_ai = request.args.get('skip_ai', 'false').lower() == 'true'

    print(f"ğŸ”„ ä¸º {persona} ç”Ÿæˆ Daily Briefing (limit={limit}, skip_ai={skip_ai})...")
    cards = build_intelligence_cards(limit=limit, skip_ai=skip_ai)
    briefing = build_daily_briefing(cards, persona=persona)

    # 4. ä¿å­˜åˆ°ç¼“å­˜
    save_daily_briefing_cache(briefing, persona=persona)

    return jsonify(briefing)


# ============ Topics / Fortress API ============

@app.route('/api/topics', methods=['GET', 'POST'])
def handle_topics():
    """å ¡å’ä¸»é¢˜ç®¡ç†"""
    if request.method == 'GET':
        return jsonify(topic_service.get_all_topics())
    elif request.method == 'POST':
        data = request.json
        if not data or 'title' not in data:
            return jsonify({'error': 'Title is required'}), 400
        new_id = topic_service.create_topic(
            data['title'], 
            data.get('description', ''), 
            data.get('channel_key')
        )
        return jsonify({'id': new_id, 'message': 'Topic created'}), 201

@app.route('/api/topics/<int:topic_id>', methods=['GET'])
def get_topic(topic_id):
    """è·å–å•ä¸ªä¸»é¢˜è¯¦æƒ…ï¼ˆå«ç‰ˆæœ¬è½´ä¸è¯æ®é“¾ï¼‰"""
    topic = topic_service.get_topic_detail(topic_id)
    if not topic:
        return jsonify({'error': 'Topic not found'}), 404
    return jsonify(topic)

@app.route('/api/entities/radar', methods=['GET'])
def get_entities_radar():
    """è·å–å®ä½“é›·è¾¾æ•°æ®"""
    data = entity_service.get_radar_data()
    return success(data={"entities": data}, meta={"count": len(data)})


# ============ å¯åŠ¨ ============

if __name__ == '__main__':
    # å…¨é‡åˆå§‹åŒ– (Railway PostgreSQL)
    from database import init_db
    init_db()
    
    print(f"âœ“ AI æ¨¡å‹: {DEFAULT_MODEL}")
    print(f"âœ“ AI æ¥å£: {OPENAI_BASE_URL}")
    print(f"âœ“ Special æ¥å£: {SPECIAL_API_URL}")
    print(f"\nğŸš€ æœ¬åœ°è°ƒè¯•å¯åŠ¨: http://0.0.0.0:8000\n")
    
    app.run(host='0.0.0.0', port=8000, debug=True)
else:
    # ç”Ÿäº§ç¯å¢ƒ (Gunicorn å¯åŠ¨) åˆå§‹åŒ–
    from database import init_db
    init_db()
    
    # å¯åŠ¨æ—¶é¢„çƒ­ï¼šæ£€æŸ¥å¹¶åŒæ­¥ä»Šæ—¥æ•°æ®
    if os.getenv('ENABLE_STARTUP_SYNC', 'true').lower() == 'true':
        try:
            print("ğŸ”„ ç”Ÿäº§ç¯å¢ƒå¯åŠ¨é¢„çƒ­...")
            get_raw_articles_by_category()
            print("âœ… æ•°æ®é¢„çƒ­å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ å¯åŠ¨é¢„çƒ­å¤±è´¥ï¼ˆä¸å½±å“æœåŠ¡ï¼‰: {e}")
